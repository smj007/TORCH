{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced Optimizers - Pytorch",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRCreI2fh7Zj"
      },
      "source": [
        "## **Adam Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqfb-hwEasAj"
      },
      "source": [
        "import torch\n",
        "from typing import Tuple, Callable, Union, Dict, Iterable\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from torch.optim import Optimizer\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class Adam(Optimizer):\n",
        "  def __init__(self, params : Iterable[Tensor], lr : float = 1e-3, betas : Tuple[float, float] = (0.9, 0.999), eps : float = 1e-3,\n",
        "               weight_decay : float = 0.0):\n",
        "    \n",
        "    super().__init__(\n",
        "        params, default = dict(lr = lr, betas = betas, eps = eps, weight_decay = weight_decay)\n",
        "    )\n",
        "\n",
        "  def update_params(self, param : Tensor, group : Dict) -> None:\n",
        "    state = self.state[param]\n",
        "    grad = param.grad \n",
        "    state[\"step\"] += 1\n",
        "    beta1, beta2 = group[\"betas\"]\n",
        "    exp_avg = state[\"exp_avg\"] * beta1 + (grad) * (1-beta1)\n",
        "    exp_avg_sq = beta2 * state[\"exp_avg_sq\"] + (1-beta2) * grad ** 2\n",
        "\n",
        "    weight_decay = group[\"weight_decay\"]\n",
        "    if weight_decay != 0:\n",
        "      grad += weight_decay * param\n",
        "\n",
        "    param -= group[\"lr\"] * exp_avg / (exp_avg_sq.sqrt() + group[\"eps\"])  \n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self, closure : Callable[[], float] = None) -> Union[float, None]: \n",
        "    loss = 0\n",
        "    if closure is not None:\n",
        "      with torch.enable_grad():\n",
        "        loss = closure()\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      for param in group[\"params\"]:\n",
        "        if not self.state[param]:\n",
        "          state = self.state[param]\n",
        "          state[\"step\"] = 0\n",
        "          state[\"exp_avg\"] = torch.zeros_like(param)\n",
        "          state[\"exp_avg_sq\"] = torch.zeros_like(param)\n",
        "\n",
        "        grad = param.grad\n",
        "        if grad is not None:\n",
        "          self.update_params(param, group)\n",
        "\n",
        "    return loss             "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7fGZkW7mXFz"
      },
      "source": [
        "## **AdaBelief Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KY5xo9omSS1"
      },
      "source": [
        "import torch\n",
        "from typing import Tuple, Callable, Union, Dict, Iterable\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from torch.optim import Optimizer\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class AdaBelief(Optimizer):\n",
        "  def __init__(self, params : Iterable[Tensor], lr : float = 1e-3, betas : Tuple[float, float] = (0.9, 0.999), eps : float = 1e-3,\n",
        "               weight_decay : float = 0.0):\n",
        "    \n",
        "    super().__init__(\n",
        "        params, default = dict(lr = lr, betas = betas, eps = eps, weight_decay = weight_decay)\n",
        "    )\n",
        "\n",
        "  def update_params(self, param : Tensor, group : Dict) -> None:\n",
        "    state = self.state[param]\n",
        "    grad = param.grad \n",
        "    state[\"step\"] += 1\n",
        "    eps = group[\"eps\"]\n",
        "    bias_correction = 1e-7\n",
        "    beta1, beta2 = group[\"betas\"]\n",
        "    exp_avg = state[\"exp_avg\"] * beta1 + (grad) * (1-beta1)\n",
        "    exp_avg_var = beta2 * state[\"exp_avg_var\"] + (1-beta2) * (grad - exp_avg) ** 2\n",
        "\n",
        "    weight_decay = group[\"weight_decay\"]\n",
        "    if weight_decay != 0:\n",
        "      grad += weight_decay * param\n",
        "\n",
        "    #param -= group[\"lr\"] * exp_avg / (exp_avg_var.sqrt() + group[\"eps\"])  \n",
        "    #here we add a bias correction term, which is an additional difference from the behavior of Adam\n",
        "    step_size = group[\"lr\"] / bias_correction  #bias corr is any constant\n",
        "    param -= step_size * exp_avg / (exp_avg_var.sqrt() + eps)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self, closure : Callable[[], float] = None) -> Union[float, None]: \n",
        "    loss = 0\n",
        "    if closure is not None:\n",
        "      with torch.enable_grad():\n",
        "        loss = closure()\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      for param in group[\"params\"]:\n",
        "        if not self.state[param]:\n",
        "          state = self.state[param]\n",
        "          state[\"step\"] = 0\n",
        "          state[\"exp_avg\"] = torch.zeros_like(param)\n",
        "          state[\"exp_avg_var\"] = torch.zeros_like(param)\n",
        "\n",
        "        grad = param.grad\n",
        "        if grad is not None:\n",
        "          self.update_params(param, group)\n",
        "\n",
        "    return loss             "
      ],
      "execution_count": 3,
      "outputs": []
    }
  ]
}