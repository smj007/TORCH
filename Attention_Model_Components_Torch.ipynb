{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention Model Components - Torch",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPJw1k8CeAV0"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "import torch.nn as nn \n",
        "\n",
        "def scaled_dot_product_attention(query : Tensor, key : Tensor, value : Tensor) -> Tensor:\n",
        "  prod = query.bmm(key.transpose(1, 2))\n",
        "  scale = query.size(-1) ** 0.5\n",
        "  softmax = F.softmax(prod/scale, dim = -1)\n",
        "  return softmax.bmm(value)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge1K2VhlfNBy"
      },
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, dim_in : int, dim_k : int, dim_v : int):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(dim_in, dim_k)\n",
        "    self.k = nn.Linear(dim_in, dim_k)\n",
        "    self.v = nn.Linear(dim_in, dim_v)\n",
        "\n",
        "  def forward(self, query : Tensor, key : Tensor, value : Tensor) -> Tensor:\n",
        "    return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value))  "
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bV3lsBShn7u"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads : int, dim_in : int, dim_k : int, dim_v : int):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList(\n",
        "        [AttentionHead(dim_in, dim_k, dim_v) for _ in range(num_heads)]\n",
        "    )\n",
        "    self.linear = nn.Linear(num_heads * dim_v, dim_in)\n",
        "\n",
        "  def forward(self, query : Tensor, key : Tensor, value : Tensor) -> Tensor:\n",
        "    return self.linear(torch.cat([\n",
        "                 h(query, key, value) for h in self.heads\n",
        "    ], dim = -1))  "
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvmXN6K1kHgG"
      },
      "source": [
        "def positional_encoding(seq_len : int, dim_model : int, device : torch.device(\"cpu\")) -> Tensor:\n",
        "  pos = torch.arange(seq_len, dtype = torch.float, device = device).reshape(1, -1, 1) #axis 1 only\n",
        "  dim = torch.arange(dim_model, dtype = torch.float, device = device).reshape(1, 1, -1) #axis 2\n",
        "  #print(pos, dim)\n",
        "  angle = (pos/1e4) ** (dim // dim_model)\n",
        "\n",
        "  return torch.where(dim.long() % 2 == 0, -torch.sin(angle), torch.cos(angle))"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCDr7OrhoZLK"
      },
      "source": [
        "#FFN in the model\n",
        "\n",
        "def ffn(dim_input : int = 512, dim_hidden : int = 2048) -> nn.Module:\n",
        "  return nn.Sequential(\n",
        "      nn.Linear(dim_input, dim_hidden),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(dim_hidden, dim_input)\n",
        "  )"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "624twzbWvA6x"
      },
      "source": [
        "#Layer Norm \n",
        "\n",
        "class Residual(nn.Module):\n",
        "  def __init__(self, sublayer : nn.Module, dimension : int, dropout : float = 0.1):\n",
        "    super().__init__()\n",
        "    self.sublayer = sublayer\n",
        "    self.norm = nn.LayerNorm(dimension)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, *tensors : Tensor) -> Tensor:\n",
        "    return self.norm(tensors[-1] + self.dropout(self.sublayer(*tensors)))  "
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYM6dPET0QzU"
      },
      "source": [
        "class TEncoderLayer(nn.Module):\n",
        "  def __init__(self, \n",
        "               dim_model : int = 512,\n",
        "               num_heads : int = 6,\n",
        "               dim_hidden : int = 2048,\n",
        "               dropout : float = 0.1):\n",
        "    super().__init__()\n",
        "    dim_k = dim_v = dim_model // num_heads\n",
        "    self.attention = Residual(\n",
        "        MultiHeadAttention(num_heads, dim_model, dim_k, dim_v),\n",
        "        dimension = dim_model,\n",
        "        dropout = dropout\n",
        "    )\n",
        "\n",
        "    self.ffn = Residual(\n",
        "        ffn(dim_model, dim_hidden),\n",
        "        dimension = dim_model,\n",
        "        dropout = dropout\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, src : Tensor) -> Tensor:\n",
        "    src = self.attention(src, src, src)\n",
        "    return self.ffn(src)\n",
        "\n",
        "\n",
        "class TEncoder(nn.Module):\n",
        "  def __init__(self, \n",
        "               num_layers : int = 6,\n",
        "               dim_model : int = 512,\n",
        "               num_heads : int = 8,\n",
        "               dim_hidden : int = 2048,\n",
        "               dropout : float = 0.1):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([\n",
        "                                 TEncoderLayer(dim_model, num_heads, dim_hidden, dropout)\n",
        "                                 for _ in range(num_layers)\n",
        "    ])    \n",
        "\n",
        "  def forward(self, src : Tensor) -> Tensor:\n",
        "    seq_len, dimension = src.size(1), src.size(2)\n",
        "    src += positional_encoding(seq_len, dimension, device = \"cpu\")\n",
        "    for layer in self.layers:\n",
        "      src = layer(src)\n",
        "\n",
        "    return src     \n"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTezgh0z5EQx"
      },
      "source": [
        "class TDecoderLayer(nn.Module):\n",
        "  def __init__(self, \n",
        "               dim_model : int = 512,\n",
        "               num_heads : int = 6,\n",
        "               dim_hidden : int = 2048,\n",
        "               dropout : float = 0.1):\n",
        "    super().__init__()\n",
        "    dim_k = dim_v = dim_model // num_heads\n",
        "\n",
        "    self.attention1 = Residual(\n",
        "        MultiHeadAttention(num_heads, dim_model, dim_k, dim_v),\n",
        "        dimension = dim_model,\n",
        "        dropout = dropout\n",
        "    )\n",
        "\n",
        "    self.attention2 = Residual(\n",
        "        MultiHeadAttention(num_heads, dim_model, dim_k, dim_v),\n",
        "        dimension = dim_model,\n",
        "        dropout = dropout\n",
        "    )    \n",
        "\n",
        "    self.ffn = Residual(\n",
        "        ffn(dim_model, dim_hidden),\n",
        "        dimension = dim_model,\n",
        "        dropout = dropout\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, target : Tensor, mem : Tensor) -> Tensor:\n",
        "    target = self.attention1(target, target, target)\n",
        "    target = self.attention2(mem, mem, mem)\n",
        "\n",
        "    return self.ffn(target)\n",
        "\n",
        "\n",
        "class TDecoder(nn.Module):\n",
        "  def __init__(self, \n",
        "               num_layers : int = 6,\n",
        "               dim_model : int = 512,\n",
        "               num_heads : int = 8,\n",
        "               dim_hidden : int = 2048,\n",
        "               dropout : float = 0.1):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([\n",
        "                                 TDecoderLayer(dim_model, num_heads, dim_hidden, dropout)\n",
        "                                 for _ in range(num_layers)\n",
        "    ])   \n",
        "\n",
        "    self.linear = nn.Linear(dim_model, dim_model) \n",
        "\n",
        "  def forward(self, target : Tensor, mem : Tensor) -> Tensor:\n",
        "    seq_len, dimension = target.size(1), target.size(2)\n",
        "    target += positional_encoding(seq_len, dimension, device = \"cpu\")\n",
        "    for layer in self.layers:\n",
        "      target = layer(target, mem)\n",
        "\n",
        "    return F.softmax(self.linear(target), dim = -1)   "
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIcF1-KD68vY"
      },
      "source": [
        "#Wrap up in a single transformer\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        num_encoder_layers: int = 6,\n",
        "        num_decoder_layers: int = 6,\n",
        "        dim_model: int = 512, \n",
        "        num_heads: int = 6, \n",
        "        dim_hidden: int = 2048, \n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = TEncoder(\n",
        "            num_layers=num_encoder_layers,\n",
        "            dim_model=dim_model,\n",
        "            num_heads=num_heads,\n",
        "            dim_hidden=dim_hidden,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.decoder = TDecoder(\n",
        "            num_layers=num_decoder_layers,\n",
        "            dim_model=dim_model,\n",
        "            num_heads=num_heads,\n",
        "            dim_hidden=dim_hidden,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "    def forward(self, src : Tensor, target : Tensor) -> Tensor:\n",
        "      memory = self.encoder(src)\n",
        "      output = self.decoder(target, memory)\n",
        "      return output    "
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNWbCJ1p7-BH"
      },
      "source": [
        "## **Check (if model works right)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifAiz-TY72ec",
        "outputId": "24addff5-e0c7-4c46-e843-50521de4bf57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "src = torch.rand(64, 16, 512)\n",
        "target = torch.rand(64, 16, 512)\n",
        "out = Transformer()(src, target)\n",
        "print(out.shape)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 16, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lio2Q3NM8E0V"
      },
      "source": [
        "#output sequence needs to be of same dims and is verified"
      ],
      "execution_count": 86,
      "outputs": []
    }
  ]
}